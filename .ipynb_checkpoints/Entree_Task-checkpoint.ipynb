{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entree Task: Hand-Written Digit Classification with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: linear layer \n",
    "Implement the forward and backward functions for a linear layer. Please read the requirement details for Task 1 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, _m, _n):\n",
    "        '''\n",
    "        :param _m: _m is the input X hidden size\n",
    "        :param _n: _n is the output Y hidden size\n",
    "        '''\n",
    "        # \"Kaiming initialization\" is important for neural network to converge. The NN will not converge without it!\n",
    "        self.W = (np.random.uniform(low=-10000.0, high=10000.0, size = (_m, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "        self.stored_X = None\n",
    "        self.W_grad = None #record the gradient of the weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: shape(X)[0] is batch size and shape(X)[1] is the #features\n",
    "         (1) Store the input X in stored_data for Backward.\n",
    "         (2) :return: X * weights\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        ##########  Code end   ##########\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* shape(output_grad)[0] is batch size and shape(output_grad)[1] is the # output features (shape(weight)[1])\n",
    "         * 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "         * 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Linear Layer\n",
    "Check your linear forward and backward function implementations with numerical derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "m = 6\n",
    "Y_grad = np.random.rand(1, m)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = LinearLayer(n, m)\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: non-linear activation\n",
    "Implement the forward and backward functions for a nonlinear layer. Please read the requirement details for Task 2 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #sigmoid layer\n",
    "    def __init__(self):\n",
    "        self.stored_X = None # Here we should store the input matrix X for Backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         *  The input X matrix has the dimension [#samples, #features].\n",
    "         *  The output Y matrix has the same dimension as the input X.\n",
    "         *  You need to perform ReLU on each element of the input matrix to calculate the output matrix.\n",
    "         *  TODO: 1) Create an output matrix by going through each element in input and calculate relu=max(0,x) and\n",
    "         *  TODO: 2) Store the input X in self.stored_X for Backward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "         /*  grad_relu(x)=1 if relu(x)=x\n",
    "         *  grad_relu(x)=0 if relu(x)=0\n",
    "         *\n",
    "         *  The input matrix has the name \"output_grad.\" The name is confusing (it is actually the input of the function). But the name follows the convension in PyTorch.\n",
    "         *  The output matrix has the same dimension as input.\n",
    "         *  The output matrix is calculated as grad_relu(stored_X)*Y_grad.\n",
    "         *  TODO: returns the output matrix calculated above\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: ReLU \n",
    "Check your ReLU forward and backward functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "Y_grad = np.random.rand(1, n)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = ReLU()\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss function\n",
    "Implement the MSE loss function and its backward derivative. Please read the requirement details for Task 3 in the code comment and in the pdf document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    # cross entropy loss\n",
    "    # return the mse loss mean(y_j-y_pred_i)^2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stored_diff = None\n",
    "    def forward(self, prediction, groundtruth):\n",
    "        '''\n",
    "        /*  TODO: 1) Calculate stored_data=pred-truth\n",
    "         *  TODO: 2) Calculate the MSE loss as the squared sum of all the elements in the stored_data divided by the number of elements, i.e., MSE(pred, truth) = ||pred-truth||^2 / N, with N as the total number of elements in the matrix\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    # return the gradient of the input data\n",
    "    def backward(self):\n",
    "        '''\n",
    "        /* TODO: return the gradient matrix of the MSE loss\n",
    "         * The output matrix has the same dimension as the stored_data (make sure you have stored the (pred-truth) in stored_data in your forward function!)\n",
    "         * Each element (i,j) of the output matrix is calculated as grad(i,j)=2(pred(i,j)-truth(i,j))/N\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Network architecture\n",
    "Implement your own neural network architecture. Please read the requirement for Task 4 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers_arch):\n",
    "        '''\n",
    "        /*  TODO: 1) Initialize the array for input layers with the proper feature sizes specified in the input vector.\n",
    "         * For the linear layer, in each pair (in_size, out_size), the in_size is the feature size of the previous layer and the out_size is the feature size of the output (that goes to the next layer)\n",
    "         * In the linear layer, the weight should have the shape (in_size, out_size).\n",
    "         \n",
    "         *  For example, if layers_arch = [['Linear', (256, 128)], ['ReLU'], ['Linear', (128, 64)], ['ReLU'], ['Linear', (64, 32)]],\n",
    "       * \t\t\t\t\t\t\t then there are three linear layers whose weights are with shapes (256, 128), (128, 64), (64, 32),\n",
    "       * \t\t\t\t\t\t\t and there are two non-linear layers.\n",
    "         *  Attention: * The output feature size of the linear layer i should always equal to the input feature size of the linear layer i+1.\n",
    "       */\n",
    "        '''\n",
    "       \n",
    "        ########## Code start  ##########\n",
    "       \n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         * TODO: propagate the input data for the first linear layer throught all the layers in the network and return the output of the last linear layer.\n",
    "         * For implementation, you need to write a for-loop to propagate the input from the first layer to the last layer (before the loss function) by going through the forward functions of all the layers.\n",
    "         * For example, for a network with k linear layers and k-1 activation layers, the data flow is:\n",
    "         * linear[0] -> activation[0] -> linear[1] ->activation[1] -> ... -> linear[k-2] -> activation[k-2] -> linear[k-1]\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "       \n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* Propagate the gradient from the last layer to the first layer by going through the backward functions of all the layers.\n",
    "         * TODO: propagate the gradient of the output (we got from the Forward method) back throught the network and return the gradient of the first layer.\n",
    "\n",
    "         * Notice: We should use the chain rule for the backward.\n",
    "         * Notice: The order is opposite to the forward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "       \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Regression Network\n",
    "Check your network implementation with a simple regression task. Here we also provide you a sample implementation for the gradient descent algorithm, which you will find useful for your own Classifier implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    #Classifier\n",
    "    def __init__(self, layers_arch, data_function, learning_rate = 1e-3, batch_size = 32, max_epoch = 200):\n",
    "\n",
    "        input_feature_size = 2\n",
    "        output_feature_size = 2\n",
    "\n",
    "        self.train_data = []\n",
    "        self.train_label = []\n",
    "        self.test_data = []\n",
    "        self.test_label = []\n",
    "\n",
    "        self.data_function = data_function\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def dataloader(self):\n",
    "        \n",
    "        '''\n",
    "        We randomly generate the mapping: (x)->(x^3+x^2 + 1)\n",
    "        '''\n",
    "        self.train_data = np.zeros((1000,1))\n",
    "        self.train_label = np.zeros((1000, 1))\n",
    "\n",
    "        for i in range(1000):\n",
    "            self.train_data[i][0] = np.random.uniform(low=0.0, high=10000.0)/10000.0\n",
    "            self.train_label[i][0] = self.data_function(self.train_data[i][0])\n",
    "\n",
    "        self.test_data = np.zeros((200, 1))\n",
    "        self.test_label = np.zeros((200, 1))\n",
    "\n",
    "        for i in range(200):\n",
    "            self.test_data[i][0] = np.random.uniform(low=-0.0, high=10000.0) / 10000.0\n",
    "            self.test_label[i][0] = self.data_function(self.test_data[i][0])\n",
    "\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data)/self.batch_size))\n",
    "\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            batch_label = self.train_label[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            \n",
    "            '''\n",
    "            /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "            \n",
    "            ##########  Code end   ##########\n",
    "            \n",
    "        return loss/n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        prediction = self.net.forward(self.test_data)\n",
    "        loss = self.loss_function.forward(prediction, self.test_label)\n",
    "        return loss\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            train_loss = self.Train_One_Epoch()\n",
    "            test_loss = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", train_loss, \" | Test loss : \", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "# regressor\n",
    "regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "def data_function(x):\n",
    "    return np.power(x,3) + pow(x,2) + 1\n",
    "regressor = Regressor(regressor_layers_arch, data_function, learning_rate = 1e-4, batch_size = 32, max_epoch = 200)\n",
    "regressor.Train()\n",
    "\n",
    "regressor.Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Classfication Network\n",
    "Implement your own classifier with gradient descent. Please read the requirement for Task 5 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(labels, classes = 10):\n",
    "    '''\n",
    "    /*  Make the labels one-hot.\n",
    "     *  For example, if there are 5 classes {0, 1, 2, 3, 4} then\n",
    "     *  [0, 2, 4] -> [[1, 0, 0, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 1, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 0, 0, 1]]\n",
    "     */\n",
    "    '''\n",
    "    \n",
    "    ########## Code start  ##########\n",
    "    \n",
    "    ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    #Classifier\n",
    "    def __init__(self, train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch, learning_rate = 1e-3, batch_size = 32, max_epoch = 200, classes = 10):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.train_data_path = train_data_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "\n",
    "\n",
    "        self.train_data = [] #The shape of train data should be (n_samples,28^2)\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        with open(self.train_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.train_data = np.array(self.train_data)\n",
    "\n",
    "        with open(self.train_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_labels.append(int(line.strip()))\n",
    "        self.train_labels = np.array(self.train_labels)\n",
    "\n",
    "        with open(self.test_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "\n",
    "        with open(self.test_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_labels.append(int(line.strip()))\n",
    "        self.test_labels = np.array(self.test_labels)\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data) / self.batch_size))\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_label = self.train_labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_one_hot_label = One_Hot_Encode(batch_label, classes = self.classes)\n",
    "            \n",
    "            '''\n",
    "             /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "           \n",
    "            ##########  Code end   ##########\n",
    "        \n",
    "        return loss / n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        '''\n",
    "        the class with max score is our predicted label\n",
    "        '''\n",
    "        score = self.net.forward(self.test_data)\n",
    "        accuracy = 0\n",
    "        for i in range(np.shape(score)[0]):\n",
    "            one_label_list = score[i].tolist()\n",
    "            label_pred = one_label_list.index(max(one_label_list))\n",
    "            if label_pred == self.test_labels[i]:\n",
    "                accuracy = accuracy +1\n",
    "\n",
    "        accuracy = accuracy/np.shape(score)[0]\n",
    "        return accuracy\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            loss = self.Train_One_Epoch()\n",
    "            accuray = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", loss, \" | Test Accuracy : \", accuray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "That's it! Try your network on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28, 256)], ['ReLU'], ['Linear', (256, 10)]]\n",
    "cls = Classifier(train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch = classifier_layers_arch, learning_rate = 0.01, batch_size = 32, max_epoch = 200)\n",
    "cls.Train()\n",
    "cls.Test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
