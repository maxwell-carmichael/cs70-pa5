{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entree Task: Implementing Your Own Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear Layer \n",
    "Implement the forward and backward functions for a linear layer. Please read the requirement details for Task 1 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, _m, _n):\n",
    "        '''\n",
    "        :param _m: _m is the input X hidden size\n",
    "        :param _n: _n is the output Y hidden size\n",
    "        '''\n",
    "        # \"Kaiming initialization\" is important for neural network to converge. The NN will not converge without it!\n",
    "        self.W = (np.random.uniform(low=-10000.0, high=10000.0, size = (_m, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "        self.stored_X = None\n",
    "        self.W_grad = None #record the gradient of the weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: shape(X)[0] is batch size and shape(X)[1] is the #features\n",
    "         (1) Store the input X in stored_data for Backward.\n",
    "         (2) :return: X * weights\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_X = X\n",
    "        res = X @ self.W\n",
    "        return res\n",
    "        \n",
    "        ##########  Code end   ##########\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* shape(output_grad)[0] is batch size and shape(output_grad)[1] is the # output features (shape(weight)[1])\n",
    "         * 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "         * 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        curr_X = self.stored_X\n",
    "        \n",
    "        partial_W = curr_X.T @ Y_grad \n",
    "        self.W_grad = partial_W\n",
    "\n",
    "        partial_X = Y_grad @ self.W.T\n",
    "        return partial_X\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Linear Layer\n",
    "Check your linear forward and backward function implementations with numerical derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[ 0.97231337  0.05430824 -0.94654771]]\n",
      "Numerical gradient: [[ 0.97231337  0.05430824 -0.94654771]]\n",
      "Error:  2.1661894500368817e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "m = 6\n",
    "Y_grad = np.random.rand(1, m)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = LinearLayer(n, m)\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-Linear Activation\n",
    "Implement the forward and backward functions for a nonlinear layer. Please read the requirement details for Task 2 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #sigmoid layer\n",
    "    def __init__(self):\n",
    "        self.stored_X = None # Here we should store the input matrix X for Backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         *  The input X matrix has the dimension [#samples, #features].\n",
    "         *  The output Y matrix has the same dimension as the input X.\n",
    "         *  You need to perform ReLU on each element of the input matrix to calculate the output matrix.\n",
    "         *  TODO: 1) Create an output matrix by going through each element in input and calculate relu=max(0,x) and\n",
    "         *  TODO: 2) Store the input X in self.stored_X for Backward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        relu_matrix = np.zeros((X.shape))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                relu_matrix[i,j] = max(0,X[i,j])\n",
    "        self.stored_X = X\n",
    "        return relu_matrix\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "         /*  grad_relu(x)=1 if relu(x)=x\n",
    "         *  grad_relu(x)=0 if relu(x)=0\n",
    "         *\n",
    "         *  The input matrix has the name \"output_grad.\" The name is confusing (it is actually the input of the function). But the name follows the convension in PyTorch.\n",
    "         *  The output matrix has the same dimension as input.\n",
    "         *  The output matrix is calculated as grad_relu(stored_X)*Y_grad.\n",
    "         *  TODO: returns the output matrix calculated above\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        output_matrix = np.zeros((Y_grad.shape))\n",
    "        for i in range(Y_grad.shape[0]):\n",
    "            for j in range(Y_grad.shape[1]):\n",
    "                if (self.stored_X[i,j] <= 0):\n",
    "                    output_matrix[i,j] = 0\n",
    "                else:\n",
    "                    output_matrix[i,j] = Y_grad[i,j]\n",
    "        return output_matrix\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: ReLU \n",
    "Check your ReLU forward and backward functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[0.59860737 0.83637524 0.02013978]]\n",
      "Numerical gradient: [[0.59860737 0.83637524 0.02013978]]\n",
      "Error:  1.460050136348201e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "Y_grad = np.random.rand(1, n)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = ReLU()\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss Function\n",
    "Implement the MSE loss function and its backward derivative. Please read the requirement details for Task 3 in the code comment and in the pdf document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    # cross entropy loss\n",
    "    # return the mse loss mean(y_j-y_pred_i)^2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stored_diff = None\n",
    "    def forward(self, prediction, groundtruth):\n",
    "        '''\n",
    "        /*  TODO: 1) Calculate stored_data=pred-truth\n",
    "         *  TODO: 2) Calculate the MSE loss as the squared sum of all the elements in the stored_data divided by the number of elements, i.e., MSE(pred, truth) = ||pred-truth||^2 / N, with N as the total number of elements in the matrix\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_diff = prediction - groundtruth\n",
    "        loss = 0\n",
    "        for i in range(prediction.shape[0]):\n",
    "            for j in range(prediction.shape[1]):\n",
    "                element_loss = ((prediction[i,j] - groundtruth[i,j]) ** 2)\n",
    "                loss = loss + element_loss\n",
    "        N = prediction.shape[0] * prediction.shape[1]\n",
    "        mse = loss / N\n",
    "        return mse\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    # return the gradient of the input data\n",
    "    def backward(self):\n",
    "        '''\n",
    "        /* TODO: return the gradient matrix of the MSE loss\n",
    "         * The output matrix has the same dimension as the stored_data (make sure you have stored the (pred-truth) in stored_data in your forward function!)\n",
    "         * Each element (i,j) of the output matrix is calculated as grad(i,j)=2(pred(i,j)-truth(i,j))/N\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        output_matrix = np.zeros((self.stored_diff.shape))\n",
    "        for i in range(output_matrix.shape[0]):\n",
    "            for j in range(output_matrix.shape[1]):\n",
    "                N = output_matrix.shape[0] * output_matrix.shape[1]\n",
    "                output_matrix[i,j] = 2 * (self.stored_diff[i,j]) / N\n",
    "        return output_matrix\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Network Architecture\n",
    "Implement your own neural network architecture. Please read the requirement for Task 4 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers_arch):\n",
    "        '''\n",
    "        /*  TODO: 1) Initialize the array for input layers with the proper feature sizes specified in the input vector.\n",
    "         * For the linear layer, in each pair (in_size, out_size), the in_size is the feature size of the previous layer and the out_size is the feature size of the output (that goes to the next layer)\n",
    "         * In the linear layer, the weight should have the shape (in_size, out_size).\n",
    "         \n",
    "         *  For example, if layers_arch = [['Linear', (256, 128)], ['ReLU'], ['Linear', (128, 64)], ['ReLU'], ['Linear', (64, 32)]],\n",
    "       * \t\t\t\t\t\t\t then there are three linear layers whose weights are with shapes (256, 128), (128, 64), (64, 32),\n",
    "       * \t\t\t\t\t\t\t and there are two non-linear layers.\n",
    "         *  Attention: * The output feature size of the linear layer i should always equal to the input feature size of the linear layer i+1.\n",
    "       */\n",
    "        '''\n",
    "       \n",
    "        ########## Code start  ##########\n",
    "        self.layers = []\n",
    "        for one in layers_arch:\n",
    "          if (one[0] == \"Linear\"):\n",
    "            linear = LinearLayer(one[1][0],one[1][1])\n",
    "            self.layers.append(linear)\n",
    "          else:\n",
    "            relu = ReLU()\n",
    "            self.layers.append(relu)\n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         * TODO: propagate the input data for the first linear layer throught all the layers in the network and return the output of the last linear layer.\n",
    "         * For implementation, you need to write a for-loop to propagate the input from the first layer to the last layer (before the loss function) by going through the forward functions of all the layers.\n",
    "         * For example, for a network with k linear layers and k-1 activation layers, the data flow is:\n",
    "         * linear[0] -> activation[0] -> linear[1] ->activation[1] -> ... -> linear[k-2] -> activation[k-2] -> linear[k-1]\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        curr_input = X\n",
    "        for i in range(len(self.layers)):\n",
    "          output = self.layers[i].forward(curr_input)\n",
    "          curr_input = output\n",
    "        return output\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* Propagate the gradient from the last layer to the first layer by going through the backward functions of all the layers.\n",
    "         * TODO: propagate the gradient of the output (we got from the Forward method) back throught the network and return the gradient of the first layer.\n",
    "\n",
    "         * Notice: We should use the chain rule for the backward.\n",
    "         * Notice: The order is opposite to the forward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        curr_input = Y_grad\n",
    "        for i in range(len(self.layers)):\n",
    "          output = self.layers[len(self.layers)-1-i].backward(curr_input)\n",
    "          curr_input = output\n",
    "        return output\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Regression Network\n",
    "Check your network implementation with a simple regression task. Here we also provide you a sample implementation for the gradient descent algorithm, which you will find useful for your own Classifier implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    #Classifier\n",
    "    def __init__(self, layers_arch, data_function, learning_rate = 1e-3, batch_size = 32, max_epoch = 200):\n",
    "\n",
    "        input_feature_size = 2\n",
    "        output_feature_size = 2\n",
    "\n",
    "        self.train_data = []\n",
    "        self.train_label = []\n",
    "        self.test_data = []\n",
    "        self.test_label = []\n",
    "\n",
    "        self.data_function = data_function\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def dataloader(self):\n",
    "        \n",
    "        '''\n",
    "        We randomly generate the mapping: (x)->(x^3+x^2 + 1)\n",
    "        '''\n",
    "        self.train_data = np.zeros((1000,1))\n",
    "        self.train_label = np.zeros((1000, 1))\n",
    "\n",
    "        for i in range(1000):\n",
    "            self.train_data[i][0] = np.random.uniform(low=0.0, high=10000.0)/10000.0\n",
    "            self.train_label[i][0] = self.data_function(self.train_data[i][0])\n",
    "\n",
    "        self.test_data = np.zeros((200, 1))\n",
    "        self.test_label = np.zeros((200, 1))\n",
    "\n",
    "        for i in range(200):\n",
    "            self.test_data[i][0] = np.random.uniform(low=-0.0, high=10000.0) / 10000.0\n",
    "            self.test_label[i][0] = self.data_function(self.test_data[i][0])\n",
    "\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data)/self.batch_size))\n",
    "\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            batch_label = self.train_label[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            \n",
    "            '''\n",
    "            /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Sample code  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Sample code ##########\n",
    "            \n",
    "        return loss/n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        prediction = self.net.forward(self.test_data)\n",
    "        loss = self.loss_function.forward(prediction, self.test_label)\n",
    "        return loss\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            train_loss = self.Train_One_Epoch()\n",
    "            test_loss = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", train_loss, \" | Test loss : \", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  8.629154109458426  | Test loss :  9.397837125575013\n",
      "Epoch:  2 / 200  | Train loss:  7.704114748073386  | Test loss :  8.40540620792892\n",
      "Epoch:  3 / 200  | Train loss:  6.914289564617914  | Test loss :  7.552881953179521\n",
      "Epoch:  4 / 200  | Train loss:  6.2321790442792455  | Test loss :  6.812779753629925\n",
      "Epoch:  5 / 200  | Train loss:  5.637249161479606  | Test loss :  6.164379785987242\n",
      "Epoch:  6 / 200  | Train loss:  5.113900934631928  | Test loss :  5.591809158741418\n",
      "Epoch:  7 / 200  | Train loss:  4.650102518387542  | Test loss :  5.082732848328469\n",
      "Epoch:  8 / 200  | Train loss:  4.236446017494497  | Test loss :  4.627440577554752\n",
      "Epoch:  9 / 200  | Train loss:  3.865483582874993  | Test loss :  4.21819741667186\n",
      "Epoch:  10 / 200  | Train loss:  3.5312513819340525  | Test loss :  3.8487738480243117\n",
      "Epoch:  11 / 200  | Train loss:  3.2289225767721814  | Test loss :  3.514100355846213\n",
      "Epoch:  12 / 200  | Train loss:  2.954550561700537  | Test loss :  3.210009972912583\n",
      "Epoch:  13 / 200  | Train loss:  2.7048764435375263  | Test loss :  2.9330439818991563\n",
      "Epoch:  14 / 200  | Train loss:  2.477182978059394  | Test loss :  2.680303657287881\n",
      "Epoch:  15 / 200  | Train loss:  2.269182598288198  | Test loss :  2.4493360485152875\n",
      "Epoch:  16 / 200  | Train loss:  2.0789308052625883  | Test loss :  2.2380452643482647\n",
      "Epoch:  17 / 200  | Train loss:  1.9047586674750951  | Test loss :  2.04462309343058\n",
      "Epoch:  18 / 200  | Train loss:  1.745219885651687  | Test loss :  1.8674944491510919\n",
      "Epoch:  19 / 200  | Train loss:  1.5990490773548425  | Test loss :  1.7052742927103908\n",
      "Epoch:  20 / 200  | Train loss:  1.4651287852965733  | Test loss :  1.5567335203786516\n",
      "Epoch:  21 / 200  | Train loss:  1.3424633229089529  | Test loss :  1.4207719020421599\n",
      "Epoch:  22 / 200  | Train loss:  1.2301580135452421  | Test loss :  1.296396597578509\n",
      "Epoch:  23 / 200  | Train loss:  1.1274027052359508  | Test loss :  1.1827051028286892\n",
      "Epoch:  24 / 200  | Train loss:  1.0334586852790637  | Test loss :  1.078871720755\n",
      "Epoch:  25 / 200  | Train loss:  0.9476483017638215  | Test loss :  0.984136838670469\n",
      "Epoch:  26 / 200  | Train loss:  0.869346738974051  | Test loss :  0.8977984352732271\n",
      "Epoch:  27 / 200  | Train loss:  0.7979755021545581  | Test loss :  0.8192053529702318\n",
      "Epoch:  28 / 200  | Train loss:  0.7329972526064907  | Test loss :  0.7477519596777844\n",
      "Epoch:  29 / 200  | Train loss:  0.6739117023657183  | Test loss :  0.6828738956648047\n",
      "Epoch:  30 / 200  | Train loss:  0.6202523329856645  | Test loss :  0.6240446591408016\n",
      "Epoch:  31 / 200  | Train loss:  0.5715837481745567  | Test loss :  0.570772832106939\n",
      "Epoch:  32 / 200  | Train loss:  0.5274995073679677  | Test loss :  0.5225997875927588\n",
      "Epoch:  33 / 200  | Train loss:  0.48762031830214014  | Test loss :  0.47909775232986923\n",
      "Epoch:  34 / 200  | Train loss:  0.4515924924315895  | Test loss :  0.4398681263081778\n",
      "Epoch:  35 / 200  | Train loss:  0.41908658846175745  | Test loss :  0.4045399833895422\n",
      "Epoch:  36 / 200  | Train loss:  0.3897961870050881  | Test loss :  0.3727686959016892\n",
      "Epoch:  37 / 200  | Train loss:  0.3634367539457105  | Test loss :  0.3442346414603278\n",
      "Epoch:  38 / 200  | Train loss:  0.33974456195270314  | Test loss :  0.31864196264133865\n",
      "Epoch:  39 / 200  | Train loss:  0.3184756490909562  | Test loss :  0.2957173599590194\n",
      "Epoch:  40 / 200  | Train loss:  0.2994048009734013  | Test loss :  0.27520890626682804\n",
      "Epoch:  41 / 200  | Train loss:  0.28232454867579226  | Test loss :  0.25688487651383624\n",
      "Epoch:  42 / 200  | Train loss:  0.26704417896353816  | Test loss :  0.24053259106186317\n",
      "Epoch:  43 / 200  | Train loss:  0.2533887565018599  | Test loss :  0.225957273764412\n",
      "Epoch:  44 / 200  | Train loss:  0.24119815985373708  | Test loss :  0.21298092797015536\n",
      "Epoch:  45 / 200  | Train loss:  0.2303261344089635  | Test loss :  0.2014412347542728\n",
      "Epoch:  46 / 200  | Train loss:  0.22063936610275153  | Test loss :  0.1911904781864103\n",
      "Epoch:  47 / 200  | Train loss:  0.2120165800215786  | Test loss :  0.18209450247419132\n",
      "Epoch:  48 / 200  | Train loss:  0.2043476678831209  | Test loss :  0.17403170551061123\n",
      "Epoch:  49 / 200  | Train loss:  0.19753284802128554  | Test loss :  0.16689207281345822\n",
      "Epoch:  50 / 200  | Train loss:  0.19148186099247216  | Test loss :  0.16057625516489427\n",
      "Epoch:  51 / 200  | Train loss:  0.18611320331392125  | Test loss :  0.15499469251018752\n",
      "Epoch:  52 / 200  | Train loss:  0.1813534012026943  | Test loss :  0.1500667859102305\n",
      "Epoch:  53 / 200  | Train loss:  0.17713632554456  | Test loss :  0.1457201186025737\n",
      "Epoch:  54 / 200  | Train loss:  0.1734025487146928  | Test loss :  0.14188972653788234\n",
      "Epoch:  55 / 200  | Train loss:  0.17009874331616556  | Test loss :  0.1385174181408946\n",
      "Epoch:  56 / 200  | Train loss:  0.16717712240976734  | Test loss :  0.1355511425071171\n",
      "Epoch:  57 / 200  | Train loss:  0.16459492038578516  | Test loss :  0.13294440479270364\n",
      "Epoch:  58 / 200  | Train loss:  0.16231391327662228  | Test loss :  0.13065572718464505\n",
      "Epoch:  59 / 200  | Train loss:  0.16029997702672052  | Test loss :  0.12864815354780806\n",
      "Epoch:  60 / 200  | Train loss:  0.15852268201910072  | Test loss :  0.1268887956284887\n",
      "Epoch:  61 / 200  | Train loss:  0.1569549220003258  | Test loss :  0.12534841854370546\n",
      "Epoch:  62 / 200  | Train loss:  0.15557257544131714  | Test loss :  0.12400106319341278\n",
      "Epoch:  63 / 200  | Train loss:  0.15435419731335323  | Test loss :  0.12282370319098829\n",
      "Epoch:  64 / 200  | Train loss:  0.1532807392398752  | Test loss :  0.12179593390774804\n",
      "Epoch:  65 / 200  | Train loss:  0.1523352959988033  | Test loss :  0.12089969126230163\n",
      "Epoch:  66 / 200  | Train loss:  0.15150287639076598  | Test loss :  0.12011899794833299\n",
      "Epoch:  67 / 200  | Train loss:  0.15077019655034687  | Test loss :  0.11943973487863184\n",
      "Epoch:  68 / 200  | Train loss:  0.150125493855163  | Test loss :  0.11884943572341405\n",
      "Epoch:  69 / 200  | Train loss:  0.14955835967694303  | Test loss :  0.1183371025323887\n",
      "Epoch:  70 / 200  | Train loss:  0.14905958931601374  | Test loss :  0.11789304054861616\n",
      "Epoch:  71 / 200  | Train loss:  0.14862104756258895  | Test loss :  0.11750871044459603\n",
      "Epoch:  72 / 200  | Train loss:  0.14823554843236092  | Test loss :  0.11717659633445274\n",
      "Epoch:  73 / 200  | Train loss:  0.1478967477280061  | Test loss :  0.11689008803834716\n",
      "Epoch:  74 / 200  | Train loss:  0.14759904718065567  | Test loss :  0.11664337619459834\n",
      "Epoch:  75 / 200  | Train loss:  0.1473375090248397  | Test loss :  0.11643135893015058\n",
      "Epoch:  76 / 200  | Train loss:  0.1471077799559245  | Test loss :  0.11624955890998105\n",
      "Epoch:  77 / 200  | Train loss:  0.14690602350993184  | Test loss :  0.11609404969017831\n",
      "Epoch:  78 / 200  | Train loss:  0.1467288599913901  | Test loss :  0.11596139039729919\n",
      "Epoch:  79 / 200  | Train loss:  0.1465733131552507  | Test loss :  0.11584856784802353\n",
      "Epoch:  80 / 200  | Train loss:  0.1464367629237914  | Test loss :  0.11575294530801067\n",
      "Epoch:  81 / 200  | Train loss:  0.14631690348882032  | Test loss :  0.11567221716729886\n",
      "Epoch:  82 / 200  | Train loss:  0.14621170621349194  | Test loss :  0.11560436888174078\n",
      "Epoch:  83 / 200  | Train loss:  0.14611938680681844  | Test loss :  0.11554764159606622\n",
      "Epoch:  84 / 200  | Train loss:  0.14603837629772337  | Test loss :  0.11550050092451053\n",
      "Epoch:  85 / 200  | Train loss:  0.14596729538450406  | Test loss :  0.1154616094198546\n",
      "Epoch:  86 / 200  | Train loss:  0.14590493178011862  | Test loss :  0.1154298023115304\n",
      "Epoch:  87 / 200  | Train loss:  0.14585022021408767  | Test loss :  0.1154040661385209\n",
      "Epoch:  88 / 200  | Train loss:  0.1458022247883015  | Test loss :  0.11538351994345306\n",
      "Epoch:  89 / 200  | Train loss:  0.14576012341693612  | Test loss :  0.11536739873091825\n",
      "Epoch:  90 / 200  | Train loss:  0.14572319411030438  | Test loss :  0.11535503892596825\n",
      "Epoch:  91 / 200  | Train loss:  0.14569080288907058  | Test loss :  0.11534586559826046\n",
      "Epoch:  92 / 200  | Train loss:  0.14566239313910867  | Test loss :  0.11533938124376857\n",
      "Epoch:  93 / 200  | Train loss:  0.14563747623862958  | Test loss :  0.11533515593959802\n",
      "Epoch:  94 / 200  | Train loss:  0.14561562330828123  | Test loss :  0.11533281870855286\n",
      "Epoch:  95 / 200  | Train loss:  0.14559645795194642  | Test loss :  0.11533204994889486\n",
      "Epoch:  96 / 200  | Train loss:  0.14557964987113825  | Test loss :  0.11533257480148212\n",
      "Epoch:  97 / 200  | Train loss:  0.145564909249396  | Test loss :  0.11533415734136032\n",
      "Epoch:  98 / 200  | Train loss:  0.14555198181509402  | Test loss :  0.11533659549409638\n",
      "Epoch:  99 / 200  | Train loss:  0.1455406445017418  | Test loss :  0.11533971658888444\n",
      "Epoch:  100 / 200  | Train loss:  0.1455307016343191  | Test loss :  0.11534337347084665\n",
      "Epoch:  101 / 200  | Train loss:  0.14552198157858154  | Test loss :  0.11534744110416645\n",
      "Epoch:  102 / 200  | Train loss:  0.1455143337977067  | Test loss :  0.11535181360584358\n",
      "Epoch:  103 / 200  | Train loss:  0.1455076262672304  | Test loss :  0.11535640165706562\n",
      "Epoch:  104 / 200  | Train loss:  0.14550174320504417  | Test loss :  0.11536113024556222\n",
      "Epoch:  105 / 200  | Train loss:  0.145496583078372  | Test loss :  0.11536593669793054\n",
      "Epoch:  106 / 200  | Train loss:  0.1454920568541868  | Test loss :  0.11537076896588362\n",
      "Epoch:  107 / 200  | Train loss:  0.14548808646354466  | Test loss :  0.1153755841347528\n",
      "Epoch:  108 / 200  | Train loss:  0.1454846034538535  | Test loss :  0.11538034712642554\n",
      "Epoch:  109 / 200  | Train loss:  0.14548154780621872  | Test loss :  0.11538502957230932\n",
      "Epoch:  110 / 200  | Train loss:  0.14547886689776163  | Test loss :  0.1153896088348935\n",
      "Epoch:  111 / 200  | Train loss:  0.14547651459123379  | Test loss :  0.1153940671591216\n",
      "Epoch:  112 / 200  | Train loss:  0.14547445043638768  | Test loss :  0.11539839093710101\n",
      "Epoch:  113 / 200  | Train loss:  0.14547263896944826  | Test loss :  0.11540257007170991\n",
      "Epoch:  114 / 200  | Train loss:  0.14547104909868477  | Test loss :  0.11540659742645845\n",
      "Epoch:  115 / 200  | Train loss:  0.145469653565542  | Test loss :  0.11541046835052597\n",
      "Epoch:  116 / 200  | Train loss:  0.14546842847207278  | Test loss :  0.11541418026928292\n",
      "Epoch:  117 / 200  | Train loss:  0.1454673528665404  | Test loss :  0.11541773233181413\n",
      "Epoch:  118 / 200  | Train loss:  0.14546640838005315  | Test loss :  0.11542112510802685\n",
      "Epoch:  119 / 200  | Train loss:  0.14546557890796308  | Test loss :  0.11542436032885867\n",
      "Epoch:  120 / 200  | Train loss:  0.145464850330529  | Test loss :  0.1154274406639182\n",
      "Epoch:  121 / 200  | Train loss:  0.1454642102680161  | Test loss :  0.1154303695316091\n",
      "Epoch:  122 / 200  | Train loss:  0.14546364786599478  | Test loss :  0.11543315093741681\n",
      "Epoch:  123 / 200  | Train loss:  0.14546315360712284  | Test loss :  0.11543578933658855\n",
      "Epoch:  124 / 200  | Train loss:  0.1454627191461491  | Test loss :  0.11543828951791563\n",
      "Epoch:  125 / 200  | Train loss:  0.14546233716527793  | Test loss :  0.11544065650575525\n",
      "Epoch:  126 / 200  | Train loss:  0.14546200124738543  | Test loss :  0.1154428954777897\n",
      "Epoch:  127 / 200  | Train loss:  0.14546170576488646  | Test loss :  0.11544501169635504\n",
      "Epoch:  128 / 200  | Train loss:  0.14546144578232195  | Test loss :  0.11544701045144022\n",
      "Epoch:  129 / 200  | Train loss:  0.14546121697097436  | Test loss :  0.11544889701371867\n",
      "Epoch:  130 / 200  | Train loss:  0.1454610155340265  | Test loss :  0.11545067659617692\n",
      "Epoch:  131 / 200  | Train loss:  0.14546083814096303  | Test loss :  0.11545235432310419\n",
      "Epoch:  132 / 200  | Train loss:  0.1454606818700726  | Test loss :  0.11545393520536183\n",
      "Epoch:  133 / 200  | Train loss:  0.14546054415805157  | Test loss :  0.11545542412100454\n",
      "Epoch:  134 / 200  | Train loss:  0.14546042275583096  | Test loss :  0.11545682580044014\n",
      "Epoch:  135 / 200  | Train loss:  0.14546031568985868  | Test loss :  0.11545814481543383\n",
      "Epoch:  136 / 200  | Train loss:  0.1454602212281632  | Test loss :  0.11545938557134977\n",
      "Epoch:  137 / 200  | Train loss:  0.1454601378506072  | Test loss :  0.11546055230211094\n",
      "Epoch:  138 / 200  | Train loss:  0.1454600642228141  | Test loss :  0.11546164906742774\n",
      "Epoch:  139 / 200  | Train loss:  0.14545999917331345  | Test loss :  0.11546267975190946\n",
      "Epoch:  140 / 200  | Train loss:  0.14545994167350718  | Test loss :  0.11546364806572663\n",
      "Epoch:  141 / 200  | Train loss:  0.14545989082010816  | Test loss :  0.11546455754653914\n",
      "Epoch:  142 / 200  | Train loss:  0.14545984581974597  | Test loss :  0.11546541156245016\n",
      "Epoch:  143 / 200  | Train loss:  0.14545980597547165  | Test loss :  0.11546621331577203\n",
      "Epoch:  144 / 200  | Train loss:  0.14545977067492608  | Test loss :  0.11546696584743524\n",
      "Epoch:  145 / 200  | Train loss:  0.14545973937996845  | Test loss :  0.11546767204188288\n",
      "Epoch:  146 / 200  | Train loss:  0.1454597116175817  | Test loss :  0.11546833463232958\n",
      "Epoch:  147 / 200  | Train loss:  0.14545968697189976  | Test loss :  0.1154689562062736\n",
      "Epoch:  148 / 200  | Train loss:  0.14545966507721583  | Test loss :  0.11546953921117584\n",
      "Epoch:  149 / 200  | Train loss:  0.14545964561185218  | Test loss :  0.11547008596022797\n",
      "Epoch:  150 / 200  | Train loss:  0.14545962829278403  | Test loss :  0.11547059863815026\n",
      "Epoch:  151 / 200  | Train loss:  0.14545961287092446  | Test loss :  0.11547107930696456\n",
      "Epoch:  152 / 200  | Train loss:  0.14545959912699  | Test loss :  0.11547152991170466\n",
      "Epoch:  153 / 200  | Train loss:  0.14545958686787286  | Test loss :  0.1154719522860273\n",
      "Epoch:  154 / 200  | Train loss:  0.14545957592346007  | Test loss :  0.11547234815769762\n",
      "Epoch:  155 / 200  | Train loss:  0.1454595661438417  | Test loss :  0.11547271915392918\n",
      "Epoch:  156 / 200  | Train loss:  0.14545955739686248  | Test loss :  0.11547306680656007\n",
      "Epoch:  157 / 200  | Train loss:  0.14545954956597293  | Test loss :  0.11547339255705685\n",
      "Epoch:  158 / 200  | Train loss:  0.14545954254834353  | Test loss :  0.11547369776133408\n",
      "Epoch:  159 / 200  | Train loss:  0.1454595362532103  | Test loss :  0.11547398369438547\n",
      "Epoch:  160 / 200  | Train loss:  0.1454595306004215  | Test loss :  0.11547425155472647\n",
      "Epoch:  161 / 200  | Train loss:  0.14545952551916302  | Test loss :  0.1154745024686418\n",
      "Epoch:  162 / 200  | Train loss:  0.14545952094683828  | Test loss :  0.11547473749424358\n",
      "Epoch:  163 / 200  | Train loss:  0.14545951682808522  | Test loss :  0.11547495762534078\n",
      "Epoch:  164 / 200  | Train loss:  0.1454595131139125  | Test loss :  0.11547516379512127\n",
      "Epoch:  165 / 200  | Train loss:  0.1454595097609412  | Test loss :  0.11547535687965428\n",
      "Epoch:  166 / 200  | Train loss:  0.14545950673073801  | Test loss :  0.11547553770121519\n",
      "Epoch:  167 / 200  | Train loss:  0.14545950398922966  | Test loss :  0.11547570703143821\n",
      "Epoch:  168 / 200  | Train loss:  0.14545950150618805  | Test loss :  0.11547586559430624\n",
      "Epoch:  169 / 200  | Train loss:  0.14545949925477766  | Test loss :  0.11547601406897769\n",
      "Epoch:  170 / 200  | Train loss:  0.14545949721115742  | Test loss :  0.11547615309246342\n",
      "Epoch:  171 / 200  | Train loss:  0.1454594953541307  | Test loss :  0.11547628326215455\n",
      "Epoch:  172 / 200  | Train loss:  0.14545949366483718  | Test loss :  0.1154764051382105\n",
      "Epoch:  173 / 200  | Train loss:  0.1454594921264819  | Test loss :  0.11547651924581333\n",
      "Epoch:  174 / 200  | Train loss:  0.1454594907240964  | Test loss :  0.11547662607729327\n",
      "Epoch:  175 / 200  | Train loss:  0.14545948944432868  | Test loss :  0.11547672609413295\n",
      "Epoch:  176 / 200  | Train loss:  0.14545948827525812  | Test loss :  0.11547681972885578\n",
      "Epoch:  177 / 200  | Train loss:  0.145459487206232  | Test loss :  0.11547690738680401\n",
      "Epoch:  178 / 200  | Train loss:  0.14545948622772206  | Test loss :  0.11547698944781343\n",
      "Epoch:  179 / 200  | Train loss:  0.14545948533119735  | Test loss :  0.11547706626778821\n",
      "Epoch:  180 / 200  | Train loss:  0.14545948450901247  | Test loss :  0.11547713818018274\n",
      "Epoch:  181 / 200  | Train loss:  0.14545948375430875  | Test loss :  0.11547720549739486\n",
      "Epoch:  182 / 200  | Train loss:  0.14545948306092682  | Test loss :  0.11547726851207622\n",
      "Epoch:  183 / 200  | Train loss:  0.1454594824233299  | Test loss :  0.115477327498362\n",
      "Epoch:  184 / 200  | Train loss:  0.14545948183653537  | Test loss :  0.11547738271302789\n",
      "Epoch:  185 / 200  | Train loss:  0.14545948129605452  | Test loss :  0.1154774343965763\n",
      "Epoch:  186 / 200  | Train loss:  0.14545948079783927  | Test loss :  0.11547748277425461\n",
      "Epoch:  187 / 200  | Train loss:  0.14545948033823497  | Test loss :  0.11547752805701451\n",
      "Epoch:  188 / 200  | Train loss:  0.1454594799139388  | Test loss :  0.11547757044240874\n",
      "Epoch:  189 / 200  | Train loss:  0.14545947952196198  | Test loss :  0.11547761011543516\n",
      "Epoch:  190 / 200  | Train loss:  0.14545947915959762  | Test loss :  0.11547764724932756\n",
      "Epoch:  191 / 200  | Train loss:  0.14545947882439103  | Test loss :  0.11547768200629767\n",
      "Epoch:  192 / 200  | Train loss:  0.145459478514114  | Test loss :  0.11547771453823154\n",
      "Epoch:  193 / 200  | Train loss:  0.14545947822674155  | Test loss :  0.11547774498734163\n",
      "Epoch:  194 / 200  | Train loss:  0.14545947796043154  | Test loss :  0.11547777348677946\n",
      "Epoch:  195 / 200  | Train loss:  0.14545947771350629  | Test loss :  0.11547780016120893\n",
      "Epoch:  196 / 200  | Train loss:  0.1454594774844363  | Test loss :  0.11547782512734486\n",
      "Epoch:  197 / 200  | Train loss:  0.14545947727182562  | Test loss :  0.11547784849445643\n",
      "Epoch:  198 / 200  | Train loss:  0.14545947707439913  | Test loss :  0.1154778703648404\n",
      "Epoch:  199 / 200  | Train loss:  0.14545947689099067  | Test loss :  0.11547789083426363\n",
      "Epoch:  200 / 200  | Train loss:  0.14545947672053264  | Test loss :  0.11547790999237813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11547790999237813"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "# regressor\n",
    "regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "def data_function(x):\n",
    "    return np.power(x,3) + pow(x,2) + 1\n",
    "regressor = Regressor(regressor_layers_arch, data_function, learning_rate = 1e-4, batch_size = 32, max_epoch = 200)\n",
    "regressor.Train()\n",
    "\n",
    "regressor.Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Classfication Network\n",
    "Implement your own classifier with gradient descent. Please read the requirement for Task 5 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(labels, classes = 10):\n",
    "    '''\n",
    "    /*  Make the labels one-hot.\n",
    "     *  For example, if there are 5 classes {0, 1, 2, 3, 4} then\n",
    "     *  [0, 2, 4] -> [[1, 0, 0, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 1, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 0, 0, 1]]\n",
    "     */\n",
    "    '''\n",
    "    \n",
    "    ########## Code start  ##########\n",
    "    encoded_matrix = np.zeros((labels.shape[0],classes))\n",
    "    for i in range(encoded_matrix.shape[0]):\n",
    "        value = labels[i]\n",
    "        for j in range(classes):\n",
    "            if value == j:\n",
    "                encoded_matrix[i,j] = 1\n",
    "            else:\n",
    "                encoded_matrix[i,j] = 0\n",
    "    return encoded_matrix\n",
    "    ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    #Classifier\n",
    "    def __init__(self, train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch, learning_rate = 1e-3, batch_size = 32, max_epoch = 200, classes = 10):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.train_data_path = train_data_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "\n",
    "\n",
    "        self.train_data = [] #The shape of train data should be (n_samples,28^2)\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        with open(self.train_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.train_data = np.array(self.train_data)\n",
    "\n",
    "        with open(self.train_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_labels.append(int(line.strip()))\n",
    "        self.train_labels = np.array(self.train_labels)\n",
    "\n",
    "        with open(self.test_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "\n",
    "        with open(self.test_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_labels.append(int(line.strip()))\n",
    "        self.test_labels = np.array(self.test_labels)\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data) / self.batch_size))\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_label = self.train_labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_one_hot_label = One_Hot_Encode(batch_label, classes = self.classes)\n",
    "            \n",
    "            '''\n",
    "             /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_one_hot_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Code end   ##########\n",
    "        \n",
    "        return loss / n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        '''\n",
    "        the class with max score is our predicted label\n",
    "        '''\n",
    "        score = self.net.forward(self.test_data)\n",
    "        accuracy = 0\n",
    "        for i in range(np.shape(score)[0]):\n",
    "            one_label_list = score[i].tolist()\n",
    "            label_pred = one_label_list.index(max(one_label_list))\n",
    "            if label_pred == self.test_labels[i]:\n",
    "                accuracy = accuracy +1\n",
    "\n",
    "        accuracy = accuracy/np.shape(score)[0]\n",
    "        return accuracy\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            loss = self.Train_One_Epoch()\n",
    "            accuray = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", loss, \" | Test Accuracy : \", accuray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "That's it! Congratulations on finishing everything. Now try your network on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  0.20915596190067895  | Test Accuracy :  0.105\n",
      "Epoch:  2 / 200  | Train loss:  0.15670766349504897  | Test Accuracy :  0.185\n",
      "Epoch:  3 / 200  | Train loss:  0.1372811528336594  | Test Accuracy :  0.245\n",
      "Epoch:  4 / 200  | Train loss:  0.12389620370617933  | Test Accuracy :  0.29\n",
      "Epoch:  5 / 200  | Train loss:  0.11410598973688997  | Test Accuracy :  0.335\n",
      "Epoch:  6 / 200  | Train loss:  0.10663216900279957  | Test Accuracy :  0.35\n",
      "Epoch:  7 / 200  | Train loss:  0.1007178329623394  | Test Accuracy :  0.365\n",
      "Epoch:  8 / 200  | Train loss:  0.09590135681825866  | Test Accuracy :  0.37\n",
      "Epoch:  9 / 200  | Train loss:  0.09187066981521375  | Test Accuracy :  0.405\n",
      "Epoch:  10 / 200  | Train loss:  0.08841334468206699  | Test Accuracy :  0.42\n",
      "Epoch:  11 / 200  | Train loss:  0.08540522947291397  | Test Accuracy :  0.44\n",
      "Epoch:  12 / 200  | Train loss:  0.08275172578861013  | Test Accuracy :  0.46\n",
      "Epoch:  13 / 200  | Train loss:  0.08037877723848309  | Test Accuracy :  0.48\n",
      "Epoch:  14 / 200  | Train loss:  0.07824345190588439  | Test Accuracy :  0.5\n",
      "Epoch:  15 / 200  | Train loss:  0.07630579549765959  | Test Accuracy :  0.505\n",
      "Epoch:  16 / 200  | Train loss:  0.07452859511033581  | Test Accuracy :  0.52\n",
      "Epoch:  17 / 200  | Train loss:  0.07289291657321095  | Test Accuracy :  0.54\n",
      "Epoch:  18 / 200  | Train loss:  0.07138174082580431  | Test Accuracy :  0.545\n",
      "Epoch:  19 / 200  | Train loss:  0.0699784121018103  | Test Accuracy :  0.56\n",
      "Epoch:  20 / 200  | Train loss:  0.06866600002022923  | Test Accuracy :  0.565\n",
      "Epoch:  21 / 200  | Train loss:  0.0674374286683864  | Test Accuracy :  0.585\n",
      "Epoch:  22 / 200  | Train loss:  0.0662837659478652  | Test Accuracy :  0.585\n",
      "Epoch:  23 / 200  | Train loss:  0.06519926069141523  | Test Accuracy :  0.61\n",
      "Epoch:  24 / 200  | Train loss:  0.06417446190136729  | Test Accuracy :  0.615\n",
      "Epoch:  25 / 200  | Train loss:  0.06320487951014012  | Test Accuracy :  0.615\n",
      "Epoch:  26 / 200  | Train loss:  0.062281581542088064  | Test Accuracy :  0.625\n",
      "Epoch:  27 / 200  | Train loss:  0.06139842145626912  | Test Accuracy :  0.63\n",
      "Epoch:  28 / 200  | Train loss:  0.060553465718283385  | Test Accuracy :  0.63\n",
      "Epoch:  29 / 200  | Train loss:  0.05974411479850834  | Test Accuracy :  0.63\n",
      "Epoch:  30 / 200  | Train loss:  0.05896851912135737  | Test Accuracy :  0.635\n",
      "Epoch:  31 / 200  | Train loss:  0.05822461415683189  | Test Accuracy :  0.64\n",
      "Epoch:  32 / 200  | Train loss:  0.05751438683104685  | Test Accuracy :  0.655\n",
      "Epoch:  33 / 200  | Train loss:  0.05683137351816212  | Test Accuracy :  0.67\n",
      "Epoch:  34 / 200  | Train loss:  0.056174256651961185  | Test Accuracy :  0.675\n",
      "Epoch:  35 / 200  | Train loss:  0.055541980157934226  | Test Accuracy :  0.67\n",
      "Epoch:  36 / 200  | Train loss:  0.05493398730790583  | Test Accuracy :  0.67\n",
      "Epoch:  37 / 200  | Train loss:  0.05434774078044781  | Test Accuracy :  0.675\n",
      "Epoch:  38 / 200  | Train loss:  0.05378148428485331  | Test Accuracy :  0.675\n",
      "Epoch:  39 / 200  | Train loss:  0.05323267136737365  | Test Accuracy :  0.68\n",
      "Epoch:  40 / 200  | Train loss:  0.052700579636726204  | Test Accuracy :  0.685\n",
      "Epoch:  41 / 200  | Train loss:  0.052183608480468414  | Test Accuracy :  0.69\n",
      "Epoch:  42 / 200  | Train loss:  0.051681866382861076  | Test Accuracy :  0.69\n",
      "Epoch:  43 / 200  | Train loss:  0.05119393446553105  | Test Accuracy :  0.695\n",
      "Epoch:  44 / 200  | Train loss:  0.050720323295416356  | Test Accuracy :  0.695\n",
      "Epoch:  45 / 200  | Train loss:  0.050260846231888257  | Test Accuracy :  0.695\n",
      "Epoch:  46 / 200  | Train loss:  0.04981293816723611  | Test Accuracy :  0.7\n",
      "Epoch:  47 / 200  | Train loss:  0.049377821500023074  | Test Accuracy :  0.7\n",
      "Epoch:  48 / 200  | Train loss:  0.048953143148848785  | Test Accuracy :  0.7\n",
      "Epoch:  49 / 200  | Train loss:  0.048539393780495446  | Test Accuracy :  0.705\n",
      "Epoch:  50 / 200  | Train loss:  0.0481353699533791  | Test Accuracy :  0.705\n",
      "Epoch:  51 / 200  | Train loss:  0.047742329026953814  | Test Accuracy :  0.705\n",
      "Epoch:  52 / 200  | Train loss:  0.04735858610228848  | Test Accuracy :  0.705\n",
      "Epoch:  53 / 200  | Train loss:  0.046983063418248476  | Test Accuracy :  0.715\n",
      "Epoch:  54 / 200  | Train loss:  0.0466163199192276  | Test Accuracy :  0.72\n",
      "Epoch:  55 / 200  | Train loss:  0.04625774138682786  | Test Accuracy :  0.725\n",
      "Epoch:  56 / 200  | Train loss:  0.045907940007336105  | Test Accuracy :  0.725\n",
      "Epoch:  57 / 200  | Train loss:  0.045565066006632286  | Test Accuracy :  0.725\n",
      "Epoch:  58 / 200  | Train loss:  0.045229422257481  | Test Accuracy :  0.725\n",
      "Epoch:  59 / 200  | Train loss:  0.04490031228079328  | Test Accuracy :  0.725\n",
      "Epoch:  60 / 200  | Train loss:  0.04457787452847305  | Test Accuracy :  0.725\n",
      "Epoch:  61 / 200  | Train loss:  0.044261455929961085  | Test Accuracy :  0.735\n",
      "Epoch:  62 / 200  | Train loss:  0.04395075368626132  | Test Accuracy :  0.735\n",
      "Epoch:  63 / 200  | Train loss:  0.043646540973896475  | Test Accuracy :  0.74\n",
      "Epoch:  64 / 200  | Train loss:  0.043349187791884014  | Test Accuracy :  0.74\n",
      "Epoch:  65 / 200  | Train loss:  0.043057526321789774  | Test Accuracy :  0.74\n",
      "Epoch:  66 / 200  | Train loss:  0.04277102346439801  | Test Accuracy :  0.745\n",
      "Epoch:  67 / 200  | Train loss:  0.042489871673135005  | Test Accuracy :  0.75\n",
      "Epoch:  68 / 200  | Train loss:  0.042213128967467865  | Test Accuracy :  0.75\n",
      "Epoch:  69 / 200  | Train loss:  0.04194118170206115  | Test Accuracy :  0.755\n",
      "Epoch:  70 / 200  | Train loss:  0.041673945861081904  | Test Accuracy :  0.755\n",
      "Epoch:  71 / 200  | Train loss:  0.04141091283478374  | Test Accuracy :  0.755\n",
      "Epoch:  72 / 200  | Train loss:  0.041152455581887085  | Test Accuracy :  0.76\n",
      "Epoch:  73 / 200  | Train loss:  0.04089885808712293  | Test Accuracy :  0.76\n",
      "Epoch:  74 / 200  | Train loss:  0.040648604451915545  | Test Accuracy :  0.76\n",
      "Epoch:  75 / 200  | Train loss:  0.04040326671328846  | Test Accuracy :  0.765\n",
      "Epoch:  76 / 200  | Train loss:  0.04016175409987825  | Test Accuracy :  0.77\n",
      "Epoch:  77 / 200  | Train loss:  0.03992404596908494  | Test Accuracy :  0.775\n",
      "Epoch:  78 / 200  | Train loss:  0.03969092334196287  | Test Accuracy :  0.775\n",
      "Epoch:  79 / 200  | Train loss:  0.039461955070507364  | Test Accuracy :  0.78\n",
      "Epoch:  80 / 200  | Train loss:  0.039236790833743866  | Test Accuracy :  0.78\n",
      "Epoch:  81 / 200  | Train loss:  0.03901568280838541  | Test Accuracy :  0.785\n",
      "Epoch:  82 / 200  | Train loss:  0.0387975872123785  | Test Accuracy :  0.785\n",
      "Epoch:  83 / 200  | Train loss:  0.03858272967477793  | Test Accuracy :  0.79\n",
      "Epoch:  84 / 200  | Train loss:  0.03837098625116239  | Test Accuracy :  0.79\n",
      "Epoch:  85 / 200  | Train loss:  0.03816270153623613  | Test Accuracy :  0.79\n",
      "Epoch:  86 / 200  | Train loss:  0.03795789363938814  | Test Accuracy :  0.79\n",
      "Epoch:  87 / 200  | Train loss:  0.03775610384904521  | Test Accuracy :  0.79\n",
      "Epoch:  88 / 200  | Train loss:  0.037557152408656036  | Test Accuracy :  0.79\n",
      "Epoch:  89 / 200  | Train loss:  0.03736048148943046  | Test Accuracy :  0.79\n",
      "Epoch:  90 / 200  | Train loss:  0.03716707861140521  | Test Accuracy :  0.795\n",
      "Epoch:  91 / 200  | Train loss:  0.03697623926779365  | Test Accuracy :  0.795\n",
      "Epoch:  92 / 200  | Train loss:  0.03678837285652065  | Test Accuracy :  0.795\n",
      "Epoch:  93 / 200  | Train loss:  0.03660258581765413  | Test Accuracy :  0.795\n",
      "Epoch:  94 / 200  | Train loss:  0.03641934764021865  | Test Accuracy :  0.795\n",
      "Epoch:  95 / 200  | Train loss:  0.03623832602937692  | Test Accuracy :  0.805\n",
      "Epoch:  96 / 200  | Train loss:  0.036059155934870545  | Test Accuracy :  0.805\n",
      "Epoch:  97 / 200  | Train loss:  0.03588203176388317  | Test Accuracy :  0.805\n",
      "Epoch:  98 / 200  | Train loss:  0.035707851264456136  | Test Accuracy :  0.805\n",
      "Epoch:  99 / 200  | Train loss:  0.035535352049944055  | Test Accuracy :  0.805\n",
      "Epoch:  100 / 200  | Train loss:  0.03536546350513663  | Test Accuracy :  0.805\n",
      "Epoch:  101 / 200  | Train loss:  0.03519767909448041  | Test Accuracy :  0.805\n",
      "Epoch:  102 / 200  | Train loss:  0.03503199637594438  | Test Accuracy :  0.805\n",
      "Epoch:  103 / 200  | Train loss:  0.034867766242158156  | Test Accuracy :  0.81\n",
      "Epoch:  104 / 200  | Train loss:  0.034705984921713576  | Test Accuracy :  0.81\n",
      "Epoch:  105 / 200  | Train loss:  0.03454561588165762  | Test Accuracy :  0.81\n",
      "Epoch:  106 / 200  | Train loss:  0.03438754460087449  | Test Accuracy :  0.81\n",
      "Epoch:  107 / 200  | Train loss:  0.03423161183773542  | Test Accuracy :  0.81\n",
      "Epoch:  108 / 200  | Train loss:  0.03407670399516643  | Test Accuracy :  0.81\n",
      "Epoch:  109 / 200  | Train loss:  0.033924472327355286  | Test Accuracy :  0.815\n",
      "Epoch:  110 / 200  | Train loss:  0.03377347694725101  | Test Accuracy :  0.82\n",
      "Epoch:  111 / 200  | Train loss:  0.03362457060158007  | Test Accuracy :  0.82\n",
      "Epoch:  112 / 200  | Train loss:  0.033476896334846316  | Test Accuracy :  0.82\n",
      "Epoch:  113 / 200  | Train loss:  0.03333135365692744  | Test Accuracy :  0.815\n",
      "Epoch:  114 / 200  | Train loss:  0.03318693463751925  | Test Accuracy :  0.815\n",
      "Epoch:  115 / 200  | Train loss:  0.03304416522640808  | Test Accuracy :  0.815\n",
      "Epoch:  116 / 200  | Train loss:  0.03290269750835255  | Test Accuracy :  0.815\n",
      "Epoch:  117 / 200  | Train loss:  0.032762219624550656  | Test Accuracy :  0.815\n",
      "Epoch:  118 / 200  | Train loss:  0.032623319122049764  | Test Accuracy :  0.82\n",
      "Epoch:  119 / 200  | Train loss:  0.03248588227386579  | Test Accuracy :  0.825\n",
      "Epoch:  120 / 200  | Train loss:  0.03234948753711487  | Test Accuracy :  0.825\n",
      "Epoch:  121 / 200  | Train loss:  0.03221486939579848  | Test Accuracy :  0.825\n",
      "Epoch:  122 / 200  | Train loss:  0.032081330968282314  | Test Accuracy :  0.825\n",
      "Epoch:  123 / 200  | Train loss:  0.03194936557681107  | Test Accuracy :  0.825\n",
      "Epoch:  124 / 200  | Train loss:  0.031818580255373353  | Test Accuracy :  0.83\n",
      "Epoch:  125 / 200  | Train loss:  0.031689492670539846  | Test Accuracy :  0.83\n",
      "Epoch:  126 / 200  | Train loss:  0.031561244486350726  | Test Accuracy :  0.83\n",
      "Epoch:  127 / 200  | Train loss:  0.03143438315485624  | Test Accuracy :  0.83\n",
      "Epoch:  128 / 200  | Train loss:  0.03130845950222771  | Test Accuracy :  0.83\n",
      "Epoch:  129 / 200  | Train loss:  0.03118370877179763  | Test Accuracy :  0.835\n",
      "Epoch:  130 / 200  | Train loss:  0.031060099269699784  | Test Accuracy :  0.84\n",
      "Epoch:  131 / 200  | Train loss:  0.030937879444161678  | Test Accuracy :  0.84\n",
      "Epoch:  132 / 200  | Train loss:  0.030816519322606577  | Test Accuracy :  0.84\n",
      "Epoch:  133 / 200  | Train loss:  0.030696255294074624  | Test Accuracy :  0.845\n",
      "Epoch:  134 / 200  | Train loss:  0.0305774120207812  | Test Accuracy :  0.845\n",
      "Epoch:  135 / 200  | Train loss:  0.030459378121891582  | Test Accuracy :  0.85\n",
      "Epoch:  136 / 200  | Train loss:  0.030342664557862813  | Test Accuracy :  0.85\n",
      "Epoch:  137 / 200  | Train loss:  0.030226929870562476  | Test Accuracy :  0.85\n",
      "Epoch:  138 / 200  | Train loss:  0.03011226444779093  | Test Accuracy :  0.85\n",
      "Epoch:  139 / 200  | Train loss:  0.029998719848858345  | Test Accuracy :  0.85\n",
      "Epoch:  140 / 200  | Train loss:  0.029885812618470977  | Test Accuracy :  0.85\n",
      "Epoch:  141 / 200  | Train loss:  0.029774110780958495  | Test Accuracy :  0.855\n",
      "Epoch:  142 / 200  | Train loss:  0.0296632346297386  | Test Accuracy :  0.855\n",
      "Epoch:  143 / 200  | Train loss:  0.029553421178251937  | Test Accuracy :  0.855\n",
      "Epoch:  144 / 200  | Train loss:  0.029444725839835487  | Test Accuracy :  0.855\n",
      "Epoch:  145 / 200  | Train loss:  0.029336727759573942  | Test Accuracy :  0.855\n",
      "Epoch:  146 / 200  | Train loss:  0.02923015983637653  | Test Accuracy :  0.85\n",
      "Epoch:  147 / 200  | Train loss:  0.02912369479652421  | Test Accuracy :  0.85\n",
      "Epoch:  148 / 200  | Train loss:  0.029018809709820115  | Test Accuracy :  0.85\n",
      "Epoch:  149 / 200  | Train loss:  0.028914550784645315  | Test Accuracy :  0.855\n",
      "Epoch:  150 / 200  | Train loss:  0.028810737890605113  | Test Accuracy :  0.855\n",
      "Epoch:  151 / 200  | Train loss:  0.02870878012922781  | Test Accuracy :  0.855\n",
      "Epoch:  152 / 200  | Train loss:  0.02860670752590642  | Test Accuracy :  0.855\n",
      "Epoch:  153 / 200  | Train loss:  0.028505619992174706  | Test Accuracy :  0.855\n",
      "Epoch:  154 / 200  | Train loss:  0.028405833982825094  | Test Accuracy :  0.855\n",
      "Epoch:  155 / 200  | Train loss:  0.02830620247390504  | Test Accuracy :  0.855\n",
      "Epoch:  156 / 200  | Train loss:  0.02820742572382385  | Test Accuracy :  0.855\n",
      "Epoch:  157 / 200  | Train loss:  0.02810939180966921  | Test Accuracy :  0.855\n",
      "Epoch:  158 / 200  | Train loss:  0.02801241141700783  | Test Accuracy :  0.855\n",
      "Epoch:  159 / 200  | Train loss:  0.027915845861129534  | Test Accuracy :  0.855\n",
      "Epoch:  160 / 200  | Train loss:  0.027820231890383153  | Test Accuracy :  0.855\n",
      "Epoch:  161 / 200  | Train loss:  0.027725212908605024  | Test Accuracy :  0.855\n",
      "Epoch:  162 / 200  | Train loss:  0.027631257934556754  | Test Accuracy :  0.855\n",
      "Epoch:  163 / 200  | Train loss:  0.027537383006876002  | Test Accuracy :  0.855\n",
      "Epoch:  164 / 200  | Train loss:  0.027445036431455443  | Test Accuracy :  0.855\n",
      "Epoch:  165 / 200  | Train loss:  0.027353017942864693  | Test Accuracy :  0.855\n",
      "Epoch:  166 / 200  | Train loss:  0.027261741345813273  | Test Accuracy :  0.855\n",
      "Epoch:  167 / 200  | Train loss:  0.027171098495953994  | Test Accuracy :  0.855\n",
      "Epoch:  168 / 200  | Train loss:  0.02708116692835487  | Test Accuracy :  0.855\n",
      "Epoch:  169 / 200  | Train loss:  0.026991839558017233  | Test Accuracy :  0.855\n",
      "Epoch:  170 / 200  | Train loss:  0.026903015198091865  | Test Accuracy :  0.855\n",
      "Epoch:  171 / 200  | Train loss:  0.02681510472409797  | Test Accuracy :  0.855\n",
      "Epoch:  172 / 200  | Train loss:  0.026727455596365895  | Test Accuracy :  0.855\n",
      "Epoch:  173 / 200  | Train loss:  0.02664046124621634  | Test Accuracy :  0.855\n",
      "Epoch:  174 / 200  | Train loss:  0.026554158036814605  | Test Accuracy :  0.855\n",
      "Epoch:  175 / 200  | Train loss:  0.02646810876554369  | Test Accuracy :  0.855\n",
      "Epoch:  176 / 200  | Train loss:  0.02638306919568364  | Test Accuracy :  0.855\n",
      "Epoch:  177 / 200  | Train loss:  0.02629863246701265  | Test Accuracy :  0.855\n",
      "Epoch:  178 / 200  | Train loss:  0.026214889642929208  | Test Accuracy :  0.855\n",
      "Epoch:  179 / 200  | Train loss:  0.026131704539669412  | Test Accuracy :  0.855\n",
      "Epoch:  180 / 200  | Train loss:  0.026048895677312566  | Test Accuracy :  0.855\n",
      "Epoch:  181 / 200  | Train loss:  0.02596672960418761  | Test Accuracy :  0.855\n",
      "Epoch:  182 / 200  | Train loss:  0.025885194899074947  | Test Accuracy :  0.855\n",
      "Epoch:  183 / 200  | Train loss:  0.02580449041013307  | Test Accuracy :  0.855\n",
      "Epoch:  184 / 200  | Train loss:  0.025724304364128522  | Test Accuracy :  0.855\n",
      "Epoch:  185 / 200  | Train loss:  0.025644499432176258  | Test Accuracy :  0.855\n",
      "Epoch:  186 / 200  | Train loss:  0.025565566609377744  | Test Accuracy :  0.855\n",
      "Epoch:  187 / 200  | Train loss:  0.025487169137504474  | Test Accuracy :  0.855\n",
      "Epoch:  188 / 200  | Train loss:  0.02540912433801938  | Test Accuracy :  0.855\n",
      "Epoch:  189 / 200  | Train loss:  0.025331826151270644  | Test Accuracy :  0.855\n",
      "Epoch:  190 / 200  | Train loss:  0.02525474109912518  | Test Accuracy :  0.855\n",
      "Epoch:  191 / 200  | Train loss:  0.025178750645503387  | Test Accuracy :  0.855\n",
      "Epoch:  192 / 200  | Train loss:  0.025102893739884193  | Test Accuracy :  0.855\n",
      "Epoch:  193 / 200  | Train loss:  0.02502741384357041  | Test Accuracy :  0.86\n",
      "Epoch:  194 / 200  | Train loss:  0.02495286360717207  | Test Accuracy :  0.86\n",
      "Epoch:  195 / 200  | Train loss:  0.02487818453468049  | Test Accuracy :  0.865\n",
      "Epoch:  196 / 200  | Train loss:  0.024804491567601512  | Test Accuracy :  0.865\n",
      "Epoch:  197 / 200  | Train loss:  0.024731006599610892  | Test Accuracy :  0.865\n",
      "Epoch:  198 / 200  | Train loss:  0.02465814345062163  | Test Accuracy :  0.865\n",
      "Epoch:  199 / 200  | Train loss:  0.024585968054168673  | Test Accuracy :  0.865\n",
      "Epoch:  200 / 200  | Train loss:  0.024513995324095744  | Test Accuracy :  0.865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28, 256)], ['ReLU'], ['Linear', (256, 10)]]\n",
    "cls = Classifier(train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch = classifier_layers_arch, learning_rate = 0.01, batch_size = 32, max_epoch = 200)\n",
    "cls.Train()\n",
    "cls.Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
